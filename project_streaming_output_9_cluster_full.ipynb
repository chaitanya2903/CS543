{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab71e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vaex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyclustering.cluster import cure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69ce2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing data before comparing it\n",
    "def normalize(data):\n",
    "    running_mat_data = data.values\n",
    "    running_mat_data_tp = np.transpose(running_mat_data)\n",
    "    running_normed_data = [np.asarray((running_mat_data_tp[i] - min_holder[i])/(max_holder[i] - min_holder[i])).reshape(-1) for i in range(115)]\n",
    "    running_normed_data = np.transpose(running_normed_data)\n",
    "    return running_normed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64133a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the distance between two vectors: used to find the closest cluster represtative to a data point\n",
    "def dist(vecA, vecB):\n",
    "    return np.sqrt(np.power(vecA - vecB, 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "031cc3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compares a datapoint to each cluster representative, and returns the cluster value that the closest representative belongs to\n",
    "#This is currently done in serial, but should really be implemented in parallel\n",
    "def get_closest_cluster(data_point, rep):\n",
    "    min_dist = float('inf')\n",
    "    for i in range(len(rep)):\n",
    "        for j in range(len(rep[i])):\n",
    "            temp_dist = dist(data_point, rep[i][j])\n",
    "            if temp_dist < min_dist:\n",
    "                min_dist = temp_dist\n",
    "                cluster_val = i\n",
    "    return cluster_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a838c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the datapoints which belong to each cluster according to whether they are malicious or not\n",
    "def get_output_data_full_streaming(chunk, mal_indicator, rep, cluster_count_benign, cluster_count_mal, ind_count):\n",
    "    for i in range(len(chunk)):\n",
    "        #Keeps a running tab of the indicator index, as this is not streamed and fits in memory\n",
    "        indicator = mal_indicator[ind_count][1]\n",
    "        ind_count += 1\n",
    "        cluster_val = get_closest_cluster(chunk[i], rep)\n",
    "        #Indicates the packet is benign\n",
    "        if indicator == 0:\n",
    "            cluster_count_benign[cluster_val] += 1\n",
    "        #Indicates the packet is malicious\n",
    "        else:\n",
    "            cluster_count_mal[cluster_val] += 1\n",
    "    return cluster_count_benign, cluster_count_mal, ind_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b13848f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the pre-computed data sample which fits in memory\n",
    "df = pd.read_csv(\"sample_3k/sample_3k_total.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "670725a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the sample data\n",
    "mat_data = df.values\n",
    "mat_data_tp = np.transpose(mat_data)\n",
    "#The minimum and maximum of the sample data is saved becaused it is used for the normalization of the streaming data later\n",
    "min_holder = np.zeros(115)\n",
    "max_holder = np.zeros(115)\n",
    "for i in range(115):\n",
    "    min_holder[i] = mat_data_tp[i].min()\n",
    "    max_holder[i] = mat_data_tp[i].max()\n",
    "normed_data = [np.asarray((mat_data_tp[i] - mat_data_tp[i].min())/(mat_data_tp[i].max() - mat_data_tp[i].min())).reshape(-1) for i in range(115)]\n",
    "normed_data = np.transpose(normed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b191ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the parameters of the CURE algorithm\n",
    "#100 clusters are used to identify the clusters with high density\n",
    "cure_algo = cure.cure(data=normed_data, number_cluster=9, number_represent_points=10, compression = 0.2, ccore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8111a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyclustering.cluster.cure.cure at 0x269b4f1cb80>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Running the algorithm\n",
    "cure_algo.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "503b2c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving the clusters\n",
    "clusters = cure_algo.get_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c058682d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2938\n",
      "1\n",
      "15051\n",
      "1\n",
      "3401\n",
      "2\n",
      "1\n",
      "15\n",
      "2590\n"
     ]
    }
   ],
   "source": [
    "#Viewing the number of datapoints in each cluster to identify the high density clusters\n",
    "for i in range(len(clusters)):\n",
    "    print(len(clusters[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e881f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the representors if each cluster\n",
    "representors = cure_algo.get_representors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0298d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the labels of the dataset (malicious or not), which fit in memory\n",
    "indicator_df = pd.read_csv(\"SSL Renegotiation_labels.csv\")\n",
    "indicator_data = indicator_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35180c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the parameters for streaming chunk size\n",
    "chunksize = 5000\n",
    "#Initializing variables which will store how many datapoints are assigned to a given cluster\n",
    "cluster_count_benign_start = np.zeros(len(representors))\n",
    "cluster_count_mal_start = np.zeros(len(representors))\n",
    "ind_count_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85706574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streaming the full dataset of an attack type in chunks, normalizing the data, and assigning each datapoint to a cluster\n",
    "for chunk in pd.read_csv(\"SSL Renegotiation_dataset-002.csv\", chunksize=chunksize):\n",
    "    normed_chunk = normalize(chunk)\n",
    "    cluster_count_benign_start, cluster_count_mal_start, ind_count_start = get_output_data_full_streaming(normed_chunk, indicator_data, representors, cluster_count_benign_start, cluster_count_mal_start, ind_count_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38066c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the results to file\n",
    "final_results = []\n",
    "final_results.append(cluster_count_benign_start)\n",
    "final_results.append(cluster_count_mal_start)\n",
    "np.savetxt(\"results/SSL_final_results.csv\", np.transpose(final_results), delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2b5821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the labels of the dataset (malicious or not), which fit in memory\n",
    "indicator_df = pd.read_csv(\"Active Wiretap_labels.csv\")\n",
    "indicator_data = indicator_df.values\n",
    "#Setting the parameters for streaming chunk size\n",
    "chunksize = 5000\n",
    "#Initializing variables which will store how many datapoints are assigned to a given cluster\n",
    "cluster_count_benign_start = np.zeros(len(representors))\n",
    "cluster_count_mal_start = np.zeros(len(representors))\n",
    "ind_count_start = 0\n",
    "#Streaming the full dataset of an attack type in chunks, normalizing the data, and assigning each datapoint to a cluster\n",
    "for chunk in pd.read_csv(\"Active Wiretap_dataset-002.csv\", chunksize=chunksize):\n",
    "    normed_chunk = normalize(chunk)\n",
    "    cluster_count_benign_start, cluster_count_mal_start, ind_count_start = get_output_data_full_streaming(normed_chunk, indicator_data, representors, cluster_count_benign_start, cluster_count_mal_start, ind_count_start)\n",
    "#Saving the results to file\n",
    "final_results = []\n",
    "final_results.append(cluster_count_benign_start)\n",
    "final_results.append(cluster_count_mal_start)\n",
    "np.savetxt(\"results/wiretap_final_results.csv\", np.transpose(final_results), delimiter = \",\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa9cd3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the labels of the dataset (malicious or not), which fit in memory\n",
    "indicator_df = pd.read_csv(\"ARP MitM_labels.csv\")\n",
    "indicator_data = indicator_df.values\n",
    "#Setting the parameters for streaming chunk size\n",
    "chunksize = 5000\n",
    "#Initializing variables which will store how many datapoints are assigned to a given cluster\n",
    "cluster_count_benign_start = np.zeros(len(representors))\n",
    "cluster_count_mal_start = np.zeros(len(representors))\n",
    "ind_count_start = 0\n",
    "#Streaming the full dataset of an attack type in chunks, normalizing the data, and assigning each datapoint to a cluster\n",
    "for chunk in pd.read_csv(\"ARP MitM_dataset-002.csv\", chunksize=chunksize):\n",
    "    normed_chunk = normalize(chunk)\n",
    "    cluster_count_benign_start, cluster_count_mal_start, ind_count_start = get_output_data_full_streaming(normed_chunk, indicator_data, representors, cluster_count_benign_start, cluster_count_mal_start, ind_count_start)\n",
    "#Saving the results to file\n",
    "final_results = []\n",
    "final_results.append(cluster_count_benign_start)\n",
    "final_results.append(cluster_count_mal_start)\n",
    "np.savetxt(\"results/ARP_final_results.csv\", np.transpose(final_results), delimiter = \",\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02f8833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the labels of the dataset (malicious or not), which fit in memory\n",
    "indicator_df = pd.read_csv(\"Fuzzing_labels.csv\")\n",
    "indicator_data = indicator_df.values\n",
    "#Setting the parameters for streaming chunk size\n",
    "chunksize = 5000\n",
    "#Initializing variables which will store how many datapoints are assigned to a given cluster\n",
    "cluster_count_benign_start = np.zeros(len(representors))\n",
    "cluster_count_mal_start = np.zeros(len(representors))\n",
    "ind_count_start = 0\n",
    "#Streaming the full dataset of an attack type in chunks, normalizing the data, and assigning each datapoint to a cluster\n",
    "for chunk in pd.read_csv(\"Fuzzing_dataset-002.csv\", chunksize=chunksize):\n",
    "    normed_chunk = normalize(chunk)\n",
    "    cluster_count_benign_start, cluster_count_mal_start, ind_count_start = get_output_data_full_streaming(normed_chunk, indicator_data, representors, cluster_count_benign_start, cluster_count_mal_start, ind_count_start)\n",
    "#Saving the results to file\n",
    "final_results = []\n",
    "final_results.append(cluster_count_benign_start)\n",
    "final_results.append(cluster_count_mal_start)\n",
    "np.savetxt(\"results/fuzzing_final_results.csv\", np.transpose(final_results), delimiter = \",\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79be4747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the labels of the dataset (malicious or not), which fit in memory\n",
    "indicator_df = pd.read_csv(\"OS Scan_labels.csv\")\n",
    "indicator_data = indicator_df.values\n",
    "#Setting the parameters for streaming chunk size\n",
    "chunksize = 5000\n",
    "#Initializing variables which will store how many datapoints are assigned to a given cluster\n",
    "cluster_count_benign_start = np.zeros(len(representors))\n",
    "cluster_count_mal_start = np.zeros(len(representors))\n",
    "ind_count_start = 0\n",
    "#Streaming the full dataset of an attack type in chunks, normalizing the data, and assigning each datapoint to a cluster\n",
    "for chunk in pd.read_csv(\"OS Scan_dataset-002.csv\", chunksize=chunksize):\n",
    "    normed_chunk = normalize(chunk)\n",
    "    cluster_count_benign_start, cluster_count_mal_start, ind_count_start = get_output_data_full_streaming(normed_chunk, indicator_data, representors, cluster_count_benign_start, cluster_count_mal_start, ind_count_start)\n",
    "#Saving the results to file\n",
    "final_results = []\n",
    "final_results.append(cluster_count_benign_start)\n",
    "final_results.append(cluster_count_mal_start)\n",
    "np.savetxt(\"results/scan_final_results.csv\", np.transpose(final_results), delimiter = \",\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eed51db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the labels of the dataset (malicious or not), which fit in memory\n",
    "indicator_df = pd.read_csv(\"SSDP Flood_labels.csv\")\n",
    "indicator_data = indicator_df.values\n",
    "#Setting the parameters for streaming chunk size\n",
    "chunksize = 5000\n",
    "#Initializing variables which will store how many datapoints are assigned to a given cluster\n",
    "cluster_count_benign_start = np.zeros(len(representors))\n",
    "cluster_count_mal_start = np.zeros(len(representors))\n",
    "ind_count_start = 0\n",
    "#Streaming the full dataset of an attack type in chunks, normalizing the data, and assigning each datapoint to a cluster\n",
    "for chunk in pd.read_csv(\"SSDP Flood_dataset-002.csv\", chunksize=chunksize):\n",
    "    normed_chunk = normalize(chunk)\n",
    "    cluster_count_benign_start, cluster_count_mal_start, ind_count_start = get_output_data_full_streaming(normed_chunk, indicator_data, representors, cluster_count_benign_start, cluster_count_mal_start, ind_count_start)\n",
    "#Saving the results to file\n",
    "final_results = []\n",
    "final_results.append(cluster_count_benign_start)\n",
    "final_results.append(cluster_count_mal_start)\n",
    "np.savetxt(\"results/SSDP_final_results.csv\", np.transpose(final_results), delimiter = \",\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37252441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the labels of the dataset (malicious or not), which fit in memory\n",
    "indicator_df = pd.read_csv(\"SYN DoS_labels.csv\")\n",
    "indicator_data = indicator_df.values\n",
    "#Setting the parameters for streaming chunk size\n",
    "chunksize = 5000\n",
    "#Initializing variables which will store how many datapoints are assigned to a given cluster\n",
    "cluster_count_benign_start = np.zeros(len(representors))\n",
    "cluster_count_mal_start = np.zeros(len(representors))\n",
    "ind_count_start = 0\n",
    "#Streaming the full dataset of an attack type in chunks, normalizing the data, and assigning each datapoint to a cluster\n",
    "for chunk in pd.read_csv(\"SYN DoS_dataset-002.csv\", chunksize=chunksize):\n",
    "    normed_chunk = normalize(chunk)\n",
    "    cluster_count_benign_start, cluster_count_mal_start, ind_count_start = get_output_data_full_streaming(normed_chunk, indicator_data, representors, cluster_count_benign_start, cluster_count_mal_start, ind_count_start)\n",
    "#Saving the results to file\n",
    "final_results = []\n",
    "final_results.append(cluster_count_benign_start)\n",
    "final_results.append(cluster_count_mal_start)\n",
    "np.savetxt(\"results/SYN_results.csv\", np.transpose(final_results), delimiter = \",\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67993679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the labels of the dataset (malicious or not), which fit in memory\n",
    "indicator_df = pd.read_csv(\"Video Injection_labels.csv\")\n",
    "indicator_data = indicator_df.values\n",
    "#Setting the parameters for streaming chunk size\n",
    "chunksize = 5000\n",
    "#Initializing variables which will store how many datapoints are assigned to a given cluster\n",
    "cluster_count_benign_start = np.zeros(len(representors))\n",
    "cluster_count_mal_start = np.zeros(len(representors))\n",
    "ind_count_start = 0\n",
    "#Streaming the full dataset of an attack type in chunks, normalizing the data, and assigning each datapoint to a cluster\n",
    "for chunk in pd.read_csv(\"Video Injection_dataset-002.csv\", chunksize=chunksize):\n",
    "    normed_chunk = normalize(chunk)\n",
    "    cluster_count_benign_start, cluster_count_mal_start, ind_count_start = get_output_data_full_streaming(normed_chunk, indicator_data, representors, cluster_count_benign_start, cluster_count_mal_start, ind_count_start)\n",
    "#Saving the results to file\n",
    "final_results = []\n",
    "final_results.append(cluster_count_benign_start)\n",
    "final_results.append(cluster_count_mal_start)\n",
    "np.savetxt(\"results/video_results.csv\", np.transpose(final_results), delimiter = \",\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
