{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab71e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vaex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyclustering.cluster import cure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59eae284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the indices of the clusters with sufficient density\n",
    "def get_rep_values(cluster, thresh):\n",
    "    holder = []\n",
    "    for i in range(len(cluster)):\n",
    "        if len(cluster[i]) >= thresh:\n",
    "            holder.append(i)\n",
    "    return holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69ce2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing data before comparing it\n",
    "def normalize(data):\n",
    "    running_mat_data = data.values\n",
    "    running_mat_data_tp = np.transpose(running_mat_data)\n",
    "    running_normed_data = [np.asarray((running_mat_data_tp[i] - min_holder[i])/(max_holder[i] - min_holder[i])).reshape(-1) for i in range(115)]\n",
    "    running_normed_data = np.transpose(running_normed_data)\n",
    "    return running_normed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64133a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the distance between two vectors: used to find the closest cluster represtative to a data point\n",
    "def dist(vecA, vecB):\n",
    "    return np.sqrt(np.power(vecA - vecB, 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031cc3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compares a datapoint to each cluster representative, and returns the cluster value that the closest representative belongs to\n",
    "#This is currently done in serial, but should really be implemented in parallel\n",
    "def get_closest_cluster(data_point, rep):\n",
    "    min_dist = float('inf')\n",
    "    for i in range(len(rep)):\n",
    "        for j in range(len(rep[i])):\n",
    "            temp_dist = dist(data_point, rep[i][j])\n",
    "            if temp_dist < min_dist:\n",
    "                min_dist = temp_dist\n",
    "                cluster_val = i\n",
    "    return cluster_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a838c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the datapoints which belong to each cluster according to whether they are malicious or not\n",
    "def get_output_data_full_streaming(chunk, rep, cluster_count):\n",
    "    for i in range(len(chunk)):\n",
    "        cluster_val = get_closest_cluster(chunk[i], rep)\n",
    "        #Indicates the packet is benign\n",
    "        cluster_count[cluster_val] += 1\n",
    "    return cluster_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b13848f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the pre-computed data sample which fits in memory\n",
    "df = pd.read_csv(\"sample_3k_mal/sample_3k_total_mal.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "670725a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the sample data\n",
    "mat_data = df.values\n",
    "mat_data_tp = np.transpose(mat_data)\n",
    "#The minimum and maximum of the sample data is saved becaused it is used for the normalization of the streaming data later\n",
    "min_holder = np.zeros(115)\n",
    "max_holder = np.zeros(115)\n",
    "for i in range(115):\n",
    "    min_holder[i] = mat_data_tp[i].min()\n",
    "    max_holder[i] = mat_data_tp[i].max()\n",
    "normed_data = [np.asarray((mat_data_tp[i] - mat_data_tp[i].min())/(mat_data_tp[i].max() - mat_data_tp[i].min())).reshape(-1) for i in range(115)]\n",
    "normed_data = np.transpose(normed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b191ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the parameters of the CURE algorithm\n",
    "#100 clusters are used to identify the clusters with high density\n",
    "cure_algo = cure.cure(data=normed_data, number_cluster=100, number_represent_points=10, compression = 0.2, ccore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8111a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyclustering.cluster.cure.cure at 0x28db499dd60>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Running the algorithm\n",
    "cure_algo.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "503b2c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving the clusters\n",
    "clusters = cure_algo.get_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c058682d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3021\n",
      "23\n",
      "2887\n",
      "6\n",
      "1\n",
      "5054\n",
      "1\n",
      "8\n",
      "5\n",
      "16\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "7\n",
      "13\n",
      "10\n",
      "3\n",
      "1\n",
      "1\n",
      "8\n",
      "1\n",
      "11179\n",
      "1\n",
      "1\n",
      "1\n",
      "32\n",
      "3\n",
      "1\n",
      "1\n",
      "4\n",
      "79\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "4\n",
      "1\n",
      "1\n",
      "1\n",
      "726\n",
      "30\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "4\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "505\n",
      "1\n",
      "103\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "107\n",
      "42\n",
      "1\n",
      "1\n",
      "6\n",
      "13\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Viewing the number of datapoints in each cluster to identify the high density clusters\n",
    "for i in range(len(clusters)):\n",
    "    print(len(clusters[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ead4a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the density threshold for what clusters to keep based on the results from above\n",
    "#Generally, top 9 density clusters as long as they include > 95% of the datapoints\n",
    "threshold = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d655767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the indices of the clusters with sufficent density\n",
    "rep_values = get_rep_values(clusters, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e881f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the representors if each cluster\n",
    "representors = cure_algo.get_representors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0298d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the cluster representators of the dense clusters\n",
    "new_rep = []\n",
    "for i in range(len(rep_values)):\n",
    "    new_rep.append(representors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35180c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the parameters for streaming chunk size\n",
    "chunksize = 5000\n",
    "#Initializing variables which will store how many datapoints are assigned to a given cluster\n",
    "cluster_count_start = np.zeros(len(new_rep))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85706574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streaming the full dataset of an attack type in chunks, normalizing the data, and assigning each datapoint to a cluster\n",
    "for chunk in pd.read_csv(\"mal_data/SSL_Mal.csv\", chunksize=chunksize):\n",
    "    normed_chunk = normalize(chunk)\n",
    "    cluster_count_start = get_output_data_full_streaming(normed_chunk, new_rep, cluster_count_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38066c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the results to file\n",
    "final_results = []\n",
    "final_results.append(cluster_count_start)\n",
    "np.savetxt(\"results/SSL_final_results_mal.csv\", np.transpose(final_results), delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591c58f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc5fcaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the parameters for streaming chunk size\n",
    "chunksize = 5000\n",
    "#Initializing variables which will store how many datapoints are assigned to a given cluster\n",
    "cluster_count_start = np.zeros(len(new_rep))\n",
    "#Streaming the full dataset of an attack type in chunks, normalizing the data, and assigning each datapoint to a cluster\n",
    "for chunk in pd.read_csv(\"mal_data/Active_Wiretap_Mal.csv\", chunksize=chunksize):\n",
    "    normed_chunk = normalize(chunk)\n",
    "    cluster_count_start = get_output_data_full_streaming(normed_chunk, new_rep, cluster_count_start)\n",
    "#Saving the results to file\n",
    "final_results = []\n",
    "final_results.append(cluster_count_start)\n",
    "np.savetxt(\"results/wiretap_final_results_mal.csv\", np.transpose(final_results), delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2a02b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2b5821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the parameters for streaming chunk size\n",
    "chunksize = 5000\n",
    "#Initializing variables which will store how many datapoints are assigned to a given cluster\n",
    "cluster_count_start = np.zeros(len(new_rep))\n",
    "#Streaming the full dataset of an attack type in chunks, normalizing the data, and assigning each datapoint to a cluster\n",
    "for chunk in pd.read_csv(\"mal_data/ARP_MITM_Mal.csv\", chunksize=chunksize):\n",
    "    normed_chunk = normalize(chunk)\n",
    "    cluster_count_start = get_output_data_full_streaming(normed_chunk, new_rep, cluster_count_start)\n",
    "#Saving the results to file\n",
    "final_results = []\n",
    "final_results.append(cluster_count_start)\n",
    "np.savetxt(\"results/ARP_final_results_mal.csv\", np.transpose(final_results), delimiter = \",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa9cd3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the parameters for streaming chunk size\n",
    "chunksize = 5000\n",
    "#Initializing variables which will store how many datapoints are assigned to a given cluster\n",
    "cluster_count_start = np.zeros(len(new_rep))\n",
    "#Streaming the full dataset of an attack type in chunks, normalizing the data, and assigning each datapoint to a cluster\n",
    "for chunk in pd.read_csv(\"mal_data/fuzzing_Mal.csv\", chunksize=chunksize):\n",
    "    normed_chunk = normalize(chunk)\n",
    "    cluster_count_start = get_output_data_full_streaming(normed_chunk, new_rep, cluster_count_start)\n",
    "#Saving the results to file\n",
    "final_results = []\n",
    "final_results.append(cluster_count_start)\n",
    "np.savetxt(\"results/fuzzing_final_results_mal.csv\", np.transpose(final_results), delimiter = \",\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02f8833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the parameters for streaming chunk size\n",
    "chunksize = 5000\n",
    "#Initializing variables which will store how many datapoints are assigned to a given cluster\n",
    "cluster_count_start = np.zeros(len(new_rep))\n",
    "#Streaming the full dataset of an attack type in chunks, normalizing the data, and assigning each datapoint to a cluster\n",
    "for chunk in pd.read_csv(\"mal_data/scan_Mal.csv\", chunksize=chunksize):\n",
    "    normed_chunk = normalize(chunk)\n",
    "    cluster_count_start = get_output_data_full_streaming(normed_chunk, new_rep, cluster_count_start)\n",
    "#Saving the results to file\n",
    "final_results = []\n",
    "final_results.append(cluster_count_start)\n",
    "np.savetxt(\"results/scan_final_results_mal.csv\", np.transpose(final_results), delimiter = \",\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79be4747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the parameters for streaming chunk size\n",
    "chunksize = 5000\n",
    "#Initializing variables which will store how many datapoints are assigned to a given cluster\n",
    "cluster_count_start = np.zeros(len(new_rep))\n",
    "#Streaming the full dataset of an attack type in chunks, normalizing the data, and assigning each datapoint to a cluster\n",
    "for chunk in pd.read_csv(\"mal_data/SSDP_Mal.csv\", chunksize=chunksize):\n",
    "    normed_chunk = normalize(chunk)\n",
    "    cluster_count_start = get_output_data_full_streaming(normed_chunk, new_rep, cluster_count_start)\n",
    "#Saving the results to file\n",
    "final_results = []\n",
    "final_results.append(cluster_count_start)\n",
    "np.savetxt(\"results/SSDP_final_results_mal.csv\", np.transpose(final_results), delimiter = \",\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eed51db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the parameters for streaming chunk size\n",
    "chunksize = 5000\n",
    "#Initializing variables which will store how many datapoints are assigned to a given cluster\n",
    "cluster_count_start = np.zeros(len(new_rep))\n",
    "#Streaming the full dataset of an attack type in chunks, normalizing the data, and assigning each datapoint to a cluster\n",
    "for chunk in pd.read_csv(\"mal_data/SYN_Mal.csv\", chunksize=chunksize):\n",
    "    normed_chunk = normalize(chunk)\n",
    "    cluster_count_start = get_output_data_full_streaming(normed_chunk, new_rep, cluster_count_start)\n",
    "#Saving the results to file\n",
    "final_results = []\n",
    "final_results.append(cluster_count_start)\n",
    "np.savetxt(\"results/SYN_final_results_mal.csv\", np.transpose(final_results), delimiter = \",\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37252441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the parameters for streaming chunk size\n",
    "chunksize = 5000\n",
    "#Initializing variables which will store how many datapoints are assigned to a given cluster\n",
    "cluster_count_start = np.zeros(len(new_rep))\n",
    "#Streaming the full dataset of an attack type in chunks, normalizing the data, and assigning each datapoint to a cluster\n",
    "for chunk in pd.read_csv(\"mal_data/video_Mal.csv\", chunksize=chunksize):\n",
    "    normed_chunk = normalize(chunk)\n",
    "    cluster_count_start = get_output_data_full_streaming(normed_chunk, new_rep, cluster_count_start)\n",
    "#Saving the results to file\n",
    "final_results = []\n",
    "final_results.append(cluster_count_start)\n",
    "np.savetxt(\"results/video_final_results_mal.csv\", np.transpose(final_results), delimiter = \",\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
